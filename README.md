# Sales-DataPipeline
***
## Project Overview
This project is an overview of an Sales Data Projection data pipeline that involves near real tine data ingestion and transformation with Change Data Capture functionality.
We will design a system using AWS services such as S3, Lambda, Glue, DynamoDB, Kinesis Stream, Kinesis Firehose and Event Bridge to ingest, transform, with change data capture functionality to load data in S3 and accessing using Athena for analytical purposes.
***


## Architectural Diagram
![SalesDataPipeline Architecture](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/SalesDataPipeline.jpg)

***

## Key Steps
### 1. Create a Table "OrdersRawTable" in DynamoDB
![DynamoDB_Table](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/DynamoDB_Table.JPG)

### 2. Create a Data Stream "kinesis-for-sales-data" in Kinesis.
![KinesisDataStream](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/KinesisDataStream.JPG)

### 3. Create a Event Bridge Pipe to ingest data from DynamoDb Stream to Kinesis Stream.
![EventBridge](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/EventBridge.JPG)

- Note: Give the attached IAM role the permission to access DynamoDB & Kinesis
    - AmazonDynamoDBFullAccess
    - AmazonKinesisFullAccess

### 4. Run the Mock data generator script to load the data in DyanomoDB.
![DataLoadCMD](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/DataLoadCMD.JPG)

### 5. Check the Data Load
- Data should be visible in DynamoDB Table "OrdersRawTable"
![DynamoDB_DataLoad](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/DynamoDB_DataLoad.JPG)

- Event Bridge should be triggerd and data should flow from DyanomoDB Stream to Kinesis Stream
![KinesisDataLoad](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/KinesisDataLoad.JPG)

### 6. Check the Change Data Capture (CDC) Events
- Before Edit the data
    - ![before1](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/before1.JPG)
    - ![before2](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/before2.JPG)
 
- After Edit the data
    - ![after1](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/after1.JPG)
    - ![after2](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/after2.JPG)
    - ![after3](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/after3.JPG)

### 7. Create Kinesis Firehose
- Create Kinesis Firehose to fetch the data from Kinesis Stream and transform it with the help of Lambda and load as batches into S3
- Kinesis Firehose
    - ![KinesisFirehose](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/KinesisFirehose.JPG)

- Lambda Function for Transformation
    - ![Lambda](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/Lambda.JPG)

- S3 Bucket "kinesis-firehose-destination-yb" for data load destination
    - ![S3Before](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/S3Before.JPG)

### 8. Generate Some more mock data in DynamoDB
- The data will be generated by mock data generator script and data will flow from DynamoDB to Kinesis Stream. from the Kinesis stream the daat will flow in Kinesis Firehose and Tranform by Lambda function and stored in the destination S3 bucket
    - ![S3After](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/S3After.JPG)

### 9. Create a Glue Crawler to crawl data from destination S3 bucket
- ![crawler](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/crawler.JPG)

    - Note: Create a Classifier and attach to crawler to load the JSON data in the correct format
    - ![claasifier](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/claasifier.JPG)

- Schema fetched by Crawler
    - ![CrawlerResult](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/CrawlerResult.JPG)

### 10. Query the data from S3 using Athena and Glue's Crawled Schema
![Athena](https://github.com/yash872/Sales-DataPipeline/blob/main/Images/Athena.JPG)




